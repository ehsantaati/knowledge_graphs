{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leverage KeyBERT, HDBSCAN and Zephyr-7B-Beta to Build a Knowledge Graph\n",
    "https://towardsdatascience.com/leverage-keybert-hdbscan-and-zephyr-7b-beta-to-build-a-knowledge-graph-33d7534ee01b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index Ipython langchain keybert transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"utils\")\n",
    "from utils.arxiv_parser import *\n",
    "from utils.llm_utils import *\n",
    "import textwrap\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from keybert.llm import TextGeneration\n",
    "from keybert import KeyLLM, KeyBERT\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/\"\n",
    "llm_path = \"../../LLMs/text-generation-webui/models/TheBloke/zephyr-7B-beta-GPTQ/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7149 with cs topic.\n",
      "There are 983 articles selected.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the data parser\n",
    "parser = ArXivDataProcessor(DATA_DIR)\n",
    "\n",
    "# Unzip the downloaded file to extract a json file in data_path\n",
    "parser.unzip_file()\n",
    "\n",
    "# Select a topic and extract the articles on that topic\n",
    "topic = 'cs'\n",
    "entries = parser.select_topic('cs')\n",
    "\n",
    "# Build a pandas dataframe with specified selections\n",
    "df = parser.select_articles(\n",
    "  entries,  # extracted articles\n",
    "  cols=['id', 'title', 'abstract'],  # features to keep\n",
    "  min_length=100,  # min tokens an abstract should have\n",
    "  max_length=120,  # max tokens an abstract should have\n",
    "  keep_abs_length=False,  # do not keep the abs_length column\n",
    "  build_corpus=False)  # do not build a corpus column\n",
    "\n",
    "# Save the selected data to a csv file 'selected_{topic}.csv', uses data_path\n",
    "parser.save_selected_data(df, topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "# prompt=f'''<|system|>\n",
    "# </s>\n",
    "# <|user|>\n",
    "# {question}</s>\n",
    "# <|assistant|>\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n"
     ]
    }
   ],
   "source": [
    "llm = AutoModelForCausalLM.from_pretrained(llm_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\") # change revision for a different branch\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_path, \n",
    "                     use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\n",
    "    model=llm,\n",
    "    tokenizer=tokenizer,\n",
    "    task='text-generation',\n",
    "    max_new_tokens=50,\n",
    "    repetition_penalty=1.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = generator(prompt)\n",
    "# print(textwrap.fill(response[0]['generated_text'],90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_keywords= \"\"\"\n",
    "<|system|>\n",
    "I have the following document:\n",
    "Semantics and Termination of Simply-Moded Logic Programs with Dynamic Scheduling\n",
    "and five candidate keywords:\n",
    "scheduling, logic, semantics, termination, moded\n",
    "\n",
    "Based on the information above, extract the keywords or the keyphrases that best describe the topic of the text.\n",
    "Follow the requirements below:\n",
    "1. Make sure to extract only the keywords or keyphrases that appear in the text.\n",
    "2. Provide five keywords or keyphrases! Do not number or label the keywords or the keyphrases!\n",
    "3. Do not include anything else besides the keywords or the keyphrases! I repeat do not include any comments!\n",
    "\n",
    "semantics, termination, simply-moded, logic programs, dynamic scheduling</s>\n",
    "\n",
    "<|user|>\n",
    "I have the following document:\n",
    "[DOCUMENT]\n",
    "and five candidate keywords:\n",
    "[CANDIDATES]\n",
    "\n",
    "Based on the information above, extract the keywords or the keyphrases that best describe the topic of the text.\n",
    "Follow the requirements below:\n",
    "1. Make sure to extract only the keywords or keyphrases that appear in the text.\n",
    "2. Provide five keywords or keyphrases! Do not number or label the keywords or the keyphrases!\n",
    "3. Do not include anything else besides the keywords or the keyphrases! I repeat do not include any comments!</s>\n",
    "\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ehsan/anaconda3/envs/nlp_toolbox_py310/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# KeyBert TextGeneration pipeline wrapper\n",
    "llm_tg = TextGeneration(generator, prompt=prompt_keywords)\n",
    "\n",
    "# Instantiate KeyBERT and specify an embedding model\n",
    "kw_model= KeyBERT(llm=llm_tg, model = \"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/ehsan/anaconda3/envs/nlp_toolbox_py310/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Retain the articles titles only for analysis\n",
    "titles_list = df.title.tolist()\n",
    "\n",
    "# Process the documents and collect the results\n",
    "titles_keys = kw_model.extract_keywords(titles_list, threshold=0.5)\n",
    "\n",
    "# Add the results to df\n",
    "df[\"titles_keys\"] = titles_keys\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_toolbox_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
